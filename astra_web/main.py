import os
from typing import Any, Type

from fastapi import Body, Depends, FastAPI, HTTPException, Query, status
from fastapi.responses import ORJSONResponse, RedirectResponse
from pydantic import BaseModel

from .auth.auth_schemes import api_key_auth
from .features.host_localized import (
    get_all_varying_features,
    get_features,
    make_feature_table,
)
from .features.schemas.io import Features, FeatureTable, FeatureTableInput
from .field.host_localized import (
    delete_field_table,
    list_field_table_file_names,
    read_field_table,
    write_field_table,
)
from .field.schemas.field_table import FieldTable
from .generator.host_localized import (
    delete_particle_distribution,
    dispatch_particle_distribution_generation,
    list_generator_ids,
    load_generator_data,
    write_particle_distribution,
)
from .generator.schemas.io import GeneratorData, GeneratorDispatchOutput, GeneratorInput
from .generator.schemas.particles import Particles
from .host_localizer import (
    HostLocalizerTypes,
    Hosts,
    LocalHostLocalizer,
    SLURMHostLocalizer,
    SLURMJobState,
)
from .host_localizer.schemas.config import SLURMConfiguration
from .host_localizer.schemas.io import JobIdsOutput
from .simulation.host_localized import (
    delete_simulation,
    dispatch_simulation_run,
    list_simulation_ids,
    load_simulation_data,
)
from .simulation.schemas.io import (
    SimulationDataWithMeta,
    SimulationDispatchOutput,
    SimulationInput,
)
from .status import DispatchStatus

tags_metadata = [
    {
        "name": "particles",
        "description": "All CRUD methods for particle distributions.\
                        Distributions are generated by ASTRA generator binary.",
    },
    {
        "name": "fields",
        "description": "All CRUD methods for fields.\
                        Fields can be uploaded for use in simulations.",
    },
    {
        "name": "simulations",
        "description": "All CRUD methods for beam dynamics simulations.\
                        Simulations are run by ASTRA binary.",
    },
    {
        "name": "features",
        "description": "Extract features across the entire database.",
    },
    {
        "name": "slurm",
        "description": "Configure and diagnose the SLURM backend.",
    },
]


app = FastAPI(
    title="ASTRA WebAPI",
    description="This is an API wrapper for the ASTRA simulation code developed \
                 by K. Floettmann at DESY Hamburg. For more information, refer to the official \
                 [website](https://www.desy.de/~mpyflo/).",
    version="3.0.0",
    contact={
        "name": "Jens Kwasniok",
        "email": "jens.kwasniok@desy.de",
    },
    root_path=os.getenv("SERVER_ROOT_PATH", ""),
    openapi_tags=tags_metadata,
    default_response_class=ORJSONResponse,
)


def _all_models(cls: Type[BaseModel] = BaseModel) -> set[Type[BaseModel]]:
    """Return a set of all subclasses of a class (default=BaseModel), recursively."""
    subs: set[Type[BaseModel]] = set(cls.__subclasses__())
    for sub in cls.__subclasses__():
        subs.update(_all_models(sub))
    return subs


def _save_alias_table(path: str) -> None:
    """Save a table of all models and their fields with aliases."""
    table: dict[str, dict[str, str | None]] = {}
    for model in _all_models():
        aliases: dict[str, str | None] = {
            f: i.alias for f, i in model.model_fields.items() if i.alias is not None
        }
        if aliases:
            table[model.__name__] = aliases

    with open(path, "w") as f:
        for model_name, fields in table.items():
            f.write(f"{model_name}:\n")
            for field_name, alias in fields.items():
                f.write(f" - {field_name}: {alias}\n")


_save_alias_table(path="alias_table.txt")


@app.get("/", include_in_schema=False)
async def root():
    return RedirectResponse(url="/docs")


@app.get(
    "/particles/ids",
    dependencies=[Depends(api_key_auth)],
    tags=["particles"],
)
def list_particle_distribution_ids(
    filter: DispatchStatus = Query(default=DispatchStatus.FINISHED),
) -> list[str]:
    """
    Returns a list of particle distribution IDs.
    """
    localizer = LocalHostLocalizer.instance()
    return list_generator_ids(localizer, filter=filter)


@app.post(
    "/particles",
    dependencies=[Depends(api_key_auth)],
    tags=["particles"],
)
async def dispatch_particle_distribution_generation_(
    generator_input: GeneratorInput,
    host: Hosts = Query(default=Hosts.LOCAL),
    timeout: int = Query(default=600),
) -> GeneratorDispatchOutput:
    local_localizer = LocalHostLocalizer.instance()
    host_localizer = HostLocalizerTypes.get_localizer(host)
    return dispatch_particle_distribution_generation(
        generator_input,
        local_localizer,
        host_localizer,
        timeout=timeout,
    )


@app.put(
    "/particles",
    dependencies=[Depends(api_key_auth)],
    tags=["particles"],
)
def upload_particle_distribution(data: Particles) -> dict[str, str]:
    localizer = LocalHostLocalizer.instance()
    gen_id = write_particle_distribution(data, localizer)
    return {"gen_id": gen_id}


@app.get(
    "/particles/{gen_id}",
    dependencies=[Depends(api_key_auth)],
    tags=["particles"],
)
def download_generator_results(gen_id: str) -> GeneratorData:
    localizer = LocalHostLocalizer.instance()
    output = load_generator_data(gen_id, localizer)
    if output is None:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Generator output for '{gen_id}' not found.",
        )
    return output


@app.delete(
    "/particles/{gen_id}",
    dependencies=[Depends(api_key_auth)],
    tags=["particles"],
)
async def delete_particle_distribution_(gen_id: str) -> None:
    localizer = LocalHostLocalizer.instance()
    blocking_links = delete_particle_distribution(gen_id, localizer)
    if blocking_links is not None:
        raise HTTPException(
            status_code=status.HTTP_409_CONFLICT,
            detail=f"Particle distribution '{gen_id}' cannot be deleted because it is referenced by: {blocking_links}",
        )


@app.get(
    "/fields",
    dependencies=[Depends(api_key_auth)],
    tags=["fields"],
)
async def list_field_table_file_names_() -> list[str]:
    localizer = LocalHostLocalizer.instance()
    return list_field_table_file_names(localizer)


@app.get(
    "/fields/{file_name}",
    dependencies=[Depends(api_key_auth)],
    tags=["fields"],
)
async def download_field_table_(file_name: str) -> FieldTable:
    localizer = LocalHostLocalizer.instance()
    try:
        table = read_field_table(file_name, localizer)
    except FileNotFoundError:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Field table '{file_name}' not found.",
        )
    return table


@app.put(
    "/fields/{file_name}",
    dependencies=[Depends(api_key_auth)],
    tags=["fields"],
)
async def upload_field_table_(file_name: str, table: FieldTable) -> None:
    localizer = LocalHostLocalizer.instance()
    try:
        write_field_table(file_name, table, localizer)
    except FileExistsError:
        raise HTTPException(
            status_code=status.HTTP_409_CONFLICT,
            detail=f"Field table '{file_name}' already exists.",
        )


@app.delete(
    "/fields/{file_name}",
    dependencies=[Depends(api_key_auth)],
    tags=["fields"],
)
async def delete_field_table_(file_name: str) -> None:
    localizer = LocalHostLocalizer.instance()
    blocking_links = delete_field_table(file_name, localizer)
    if blocking_links:
        raise HTTPException(
            status_code=status.HTTP_409_CONFLICT,
            detail=f"Field table '{file_name}' cannot be deleted because it is referenced by: {blocking_links}",
        )


@app.get(
    "/simulations/ids",
    dependencies=[Depends(api_key_auth)],
    tags=["simulations"],
)
def list_simulation_ids_(
    filter: DispatchStatus = Query(default=DispatchStatus.FINISHED),
) -> list[str]:
    """
    Returns a list of  simulation IDs.
    """
    localizer = LocalHostLocalizer.instance()
    return list_simulation_ids(localizer, filter=filter)


@app.post(
    "/simulations",
    dependencies=[Depends(api_key_auth)],
    tags=["simulations"],
)
async def dispatch_simulation(
    simulation_input: SimulationInput,
    host: Hosts = Query(default=Hosts.LOCAL),
) -> SimulationDispatchOutput:
    # local
    local_localizer = LocalHostLocalizer.instance()
    host_localizer = HostLocalizerTypes.get_localizer(host)
    return dispatch_simulation_run(simulation_input, local_localizer, host_localizer)


@app.get(
    "/simulations/{sim_id}",
    dependencies=[Depends(api_key_auth)],
    tags=["simulations"],
)
def download_simulation_data(sim_id: str) -> SimulationDataWithMeta:
    """
    Returns the output of a specific ASTRA simulation on the requested server depending
    on the given ID.
    """
    localizer = LocalHostLocalizer.instance()
    output = load_simulation_data(sim_id, localizer)
    if output is None:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Simulation '{sim_id}' not found.",
        )
    else:
        return output


@app.delete(
    "/simulations/{sim_id}",
    dependencies=[Depends(api_key_auth)],
    tags=["simulations"],
)
async def delete_simulation_(sim_id: str) -> None:
    localizer = LocalHostLocalizer.instance()
    return delete_simulation(sim_id, localizer)


# note: get is not allowed with a body, so post is used as a workaround
# see: https://datatracker.ietf.org/doc/html/rfc9110#name-get
@app.post(
    "/features",
    dependencies=[Depends(api_key_auth)],
    tags=["features"],
)
async def download_features_table(
    sim_ids: list[str] | None = Body(default=None, examples=[["sim_id_1", "sim_id_2"]]),
    features: FeatureTableInput = Body(
        ...,
        examples=[
            [
                "sim_id",
                "generator.input.particle_count",
                "simulation.output.emittance_z",
            ]
        ],
    ),
    filter_by_comment: str | None = Body(
        default=None,
        examples=["experiment v.*"],
        help="Optional regex to filter simulations by their comment.",
    ),
) -> FeatureTable:
    """
    Returns a table of requested features.

    The `sim_ids` parameter is an optional list of (finished) simulation IDs for which the features should be computed.
    If none are provided, the features will be computed for all finished simulations in the database.

    See schema `Features` for a list of all available features.
    """
    localizer = LocalHostLocalizer.instance()
    try:
        return make_feature_table(
            sim_ids,
            features,
            localizer,
            filter_by_comment=filter_by_comment,
        )
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e),
        )


# note: get is not allowed with a body, so post is used as a workaround
# see: https://datatracker.ietf.org/doc/html/rfc9110#name-get
@app.post(
    "/features/varying",
    dependencies=[Depends(api_key_auth)],
    tags=["features"],
)
async def download_varying_features(
    sim_ids: list[str] | None = Body(default=None, examples=[["sim_id_1", "sim_id_2"]]),
) -> dict[str, list[Any]]:
    """
    Returns all varying features of simulations. Features which are identical across all simulations are skipped.

    The `sim_ids` parameter is a list of (finished) simulation IDs for which the features should be computed.
    If none are provided, the features will be computed for all simulations in the database.
    """
    localizer = LocalHostLocalizer.instance()
    return get_all_varying_features(sim_ids, localizer)


# note: get is not allowed with a body, so post is used as a workaround
# see: https://datatracker.ietf.org/doc/html/rfc9110#name-get
# note: This enpoint is somewhat redundant with the above, but it is kept for discoverability and consistency.
@app.post(
    "/features/{sim_id}",
    dependencies=[Depends(api_key_auth)],
    tags=["features"],
)
async def download_all_features_for_simulation(
    sim_id: str,
) -> Features:
    """
    Returns all features for a specific simulation run.
    note: May include raw and meta data as well.
    """
    localizer = LocalHostLocalizer.instance()
    try:
        return get_features(sim_id, localizer)
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e),
        )


@app.get(
    "/slurm/configuration",
    dependencies=[Depends(api_key_auth)],
    tags=["slurm"],
)
async def download_slurm_configuration() -> SLURMConfiguration:
    slurm_localizer = SLURMHostLocalizer.instance()
    return slurm_localizer.configuration


@app.put(
    "/slurm/configuration",
    dependencies=[Depends(api_key_auth)],
    tags=["slurm"],
)
async def configure_slurm(config: SLURMConfiguration) -> None:
    slurm_localizer = SLURMHostLocalizer.instance()
    slurm_localizer.configure(config)


@app.put(
    "/slurm/configuration/user_token",
    dependencies=[Depends(api_key_auth)],
    tags=["slurm"],
)
async def update_slurm_user_token(
    value: str = Query(..., description="The new SLURM user token.")
) -> None:
    """
    Exclusively updates the SLURM user token used for authentication with the SLURM REST API and nothing more.

    Usefull in case the previous token has expired or is no longer valid.
    """
    slurm_localizer = SLURMHostLocalizer.instance()
    slurm_localizer.update_user_token(value)


@app.get(
    "/slurm/ping",
    dependencies=[Depends(api_key_auth)],
    tags=["slurm"],
)
async def ping_slurm() -> dict[str, Any]:
    slurm_localizer = SLURMHostLocalizer.instance()
    return slurm_localizer.ping()


@app.get(
    "/slurm/diagnose",
    dependencies=[Depends(api_key_auth)],
    tags=["slurm"],
)
async def diagnose_slurm() -> dict[str, Any]:
    slurm_localizer = SLURMHostLocalizer.instance()
    return slurm_localizer.diagnose()


@app.get(
    "/slurm/jobs",
    dependencies=[Depends(api_key_auth)],
    tags=["slurm"],
)
async def list_slurm_jobs(
    states: set[SLURMJobState] = Query(default=set(), alias="state"),
) -> list[dict[str, Any]]:
    """
    Lists jobs currently managed by SLURM.

    see: https://slurm.schedmd.com/rest_api.html#slurmdbV0043GetJobs
    """
    slurm_localizer = SLURMHostLocalizer.instance()
    return slurm_localizer.list_jobs(states=states)


@app.get(
    "/slurm/jobs/ids",
    dependencies=[Depends(api_key_auth)],
    tags=["slurm"],
)
async def list_slurm_managed_ids(
    state: set[SLURMJobState] = Query(default=set()),
) -> JobIdsOutput:
    """
    Lists IDs of existing generations or simulations which have been dispatched to and are still remembered by SLURM.

    note: This means that jobs which have been deleted or are too old are not included in the list.
    """
    slurm_localizer = SLURMHostLocalizer.instance()
    return slurm_localizer.list_job_ids(
        state=state,
        local_localizer=LocalHostLocalizer.instance(),
    )
